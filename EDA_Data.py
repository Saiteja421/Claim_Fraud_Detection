# -*- coding: utf-8 -*-
"""EDA_NEW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EWFYNF8WVMt1hHB6-nXGyodOSaDlHKj4

# EDA and Preprocessing

#### points  to remember while  doing EDA and preprocessing

1. The cost of misclassification is very high.
2. we want our model to be highly interpretable
3. The outliers may help for classification.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.model_selection import train_test_split

## list of csv for healthcare provider fraud detection.
list_of_csv=os.listdir()
# print(list_of_csv)

"""### Loading Train and Test data."""

# train data
# train_label=pd.read_csv('https://drive.google.com/uc?id=1RpIT7D5Omw2Wk2xyPT15Qm-M4s1j7V_J')
# train_beneficiary=pd.read_csv('https://drive.google.com/uc?id=1cskBayyKjjFI-RqUACejlw54P0Xnp0xZ')
# train_inpatient=pd.read_csv('https://drive.google.com/uc?id=1s1C4N3PJHobQ3Rz7fjcD6Am9F3iBg4ap')
# train_outpatient=pd.read_csv('https://drive.google.com/uc?id=1m0oVVMZvaj2geEp2v8GqGirs9Y1P5BCy')

# # test data
# test_label=pd.read_csv('https://drive.google.com/uc?id=1Q07zTwCIBdTWWg2S-F1fovm8OTttqT79')
# test_beneficiary=pd.read_csv('https://drive.google.com/uc?id=1A3VXWP5KyaTYeDFbB0y47Zt6wVQ057j8')
# test_inpatient=pd.read_csv('https://drive.google.com/uc?id=1ORJNu4_XTDa1oX-QciJauuaIfB6tMPVh')
# test_outpatient=pd.read_csv('https://drive.google.com/uc?id=1ORJNu4_XTDa1oX-QciJauuaIfB6tMPVh')


# # train data
# train_label=pd.read_csv('Dataset/Train-1542865627584.csv')
# train_beneficiary=pd.read_csv('Dataset/Train_Beneficiarydata-1542865627584.csv')
# train_inpatient=pd.read_csv('Dataset/Train_Inpatientdata-1542865627584.csv')
# train_outpatient=pd.read_csv('Dataset/Train_Outpatientdata-1542865627584.csv')

# # test data
# test_label=pd.read_csv('Dataset/Test-1542969243754.csv')
# test_beneficiary=pd.read_csv('Dataset/Test_Beneficiarydata-1542969243754.csv')
# test_inpatient=pd.read_csv('Dataset/Test_Inpatientdata-1542969243754.csv')
# test_outpatient=pd.read_csv('Dataset/Test_Outpatientdata-1542969243754.csv')

# train data
train_label=pd.read_csv('data/train-label.csv')
train_beneficiary=pd.read_csv('data/train-bene.csv')
train_inpatient=pd.read_csv('data/train-inp.csv')
train_outpatient=pd.read_csv('data/train-out.csv')

# test data
test_label=pd.read_csv('data/test-label.csv')
test_beneficiary=pd.read_csv('data/test-bene.csv')
test_inpatient=pd.read_csv('data/test-inp.csv')
test_outpatient=pd.read_csv('data/test-out.csv')

"""## EDA on train data

### Trian label
"""

# train_label.head(5)

#checking tain_label information
# train_label.info()

# checking if any values  is missing or null
# train_label['PotentialFraud'].isnull().any()

# checking how much data is point is fraud or not  fraud present in our data set
plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.countplot(data=train_label,x='PotentialFraud') # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of  Class Labels', fontsize=20)
plt.xlabel('Whether Potentially Fraud', size = 14)
plt.ylabel('Count of fraud', size = 14)
count=train_label['PotentialFraud'].value_counts()
no=np.round((count[0]/(count[0]+count[1]))*100,4)
yes=np.round((count[1]/(count[0]+count[1]))*100,4)
print('percentage of Fradus in data ', yes)
print('percentage of Non Frauds in data ', no)
plt.show()

"""### Observations
 1.There is no missing valus in class labels(Potential_frauds)
 
 2.Data is highly imbanced
"""

#replacing yes or no with 0 and 1
# 0 = non fruad
# 1 = fraud
train_label['PotentialFraud']=train_label['PotentialFraud'].replace('Yes',1)
train_label['PotentialFraud']=train_label['PotentialFraud'].replace('No',0)
print(train_label.head())



"""## Beneficiary data"""

# train_beneficiary.head()

# checking train_beneficiary information
# train_beneficiary.info()

#checking weather a column has Nan values or not
train_bene_col=train_beneficiary.columns
for i in train_bene_col:
    print(i,"=",train_beneficiary[i].isnull().any())

# cheking unique values in categorigal featues 
train_bene_col=train_beneficiary.columns
train_bene_col=list(train_bene_col)
train_bene_col=train_bene_col[3:-4] # removing DOB,DOD and other continues feateus.
for i in train_bene_col:
    print(i,"=",train_beneficiary[i].unique())

"""

### Preprocessing


1. only data of death column has Nan values 
2. we have to change gender to 1 and 0 from 1 and 2.(for  easy interpritaion)
3. In RenalDiseaseIndicator we have to replace Y with 1.
4. we have to make weather a beneficiary has chronic condition or not by replacing 2 with 0 (no cronic condition).From this we can add another feature called Total_chronic_condition by simply adding them."""

# replacing 2 with 0 in gender, train_beneficary[gender]
train_beneficiary['Gender']=train_beneficiary['Gender'].replace(2,0)
train_beneficiary['Gender'].head()
# replacing in test beneficary data
test_beneficiary['Gender']=test_beneficiary['Gender'].replace(2,0)
test_beneficiary['Gender'].head()

# ditribution plot of gender
plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.countplot(data=train_beneficiary,x='Gender') # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of Gender', fontsize=20)
plt.xlabel('Gender', size = 14)
plt.ylabel('Count', size = 14)
count=train_beneficiary['Gender'].value_counts()
no=np.round((count[0]/(count[0]+count[1]))*100,4)
yes=np.round((count[1]/(count[0]+count[1]))*100,4)
print('percentage of gender 1 ', yes)
print('percentage of gender O ', no)
plt.show()

"""### Observations
1. gender_0 is 57.09% and gender_1 is 42.02%
2. The gernder are balanced in beneficiary data set.

##### RenalDiseaseIndicator
"""

# replacing Y with 1 in RenalDiseaseIndicator
train_beneficiary['RenalDiseaseIndicator']=train_beneficiary['RenalDiseaseIndicator'].replace('Y',1)
# repacling in test data
test_beneficiary['RenalDiseaseIndicator']=test_beneficiary['RenalDiseaseIndicator'].replace('Y',1)
test_beneficiary['RenalDiseaseIndicator'].head()

"""##### chronic condition """

# replacing 2, with 0 in all 10 chronic condition, here we considering zero as no chornic condition
train_beneficiary['ChronicCond_Alzheimer']=train_beneficiary['ChronicCond_Alzheimer'].replace(2,0)
train_beneficiary['ChronicCond_Cancer']=train_beneficiary['ChronicCond_Cancer'].replace(2,0)
train_beneficiary['ChronicCond_Depression']=train_beneficiary['ChronicCond_Depression'].replace(2,0)
train_beneficiary['ChronicCond_Diabetes']=train_beneficiary['ChronicCond_Diabetes'].replace(2,0)
train_beneficiary['ChronicCond_Heartfailure']=train_beneficiary['ChronicCond_Heartfailure'].replace(2,0)
train_beneficiary['ChronicCond_IschemicHeart']=train_beneficiary['ChronicCond_IschemicHeart'].replace(2,0)
train_beneficiary['ChronicCond_KidneyDisease']=train_beneficiary['ChronicCond_KidneyDisease'].replace(2,0)
train_beneficiary['ChronicCond_ObstrPulmonary']=train_beneficiary['ChronicCond_ObstrPulmonary'].replace(2,0)
train_beneficiary['ChronicCond_Osteoporasis']=train_beneficiary['ChronicCond_Osteoporasis'].replace(2,0)
train_beneficiary['ChronicCond_rheumatoidarthritis']=train_beneficiary['ChronicCond_rheumatoidarthritis'].replace(2,0)
train_beneficiary['ChronicCond_stroke']=train_beneficiary['ChronicCond_stroke'].replace(2,0)

# replacing in test data
test_beneficiary['ChronicCond_Alzheimer']=test_beneficiary['ChronicCond_Alzheimer'].replace(2,0)
test_beneficiary['ChronicCond_Cancer']=test_beneficiary['ChronicCond_Cancer'].replace(2,0)
test_beneficiary['ChronicCond_Depression']=test_beneficiary['ChronicCond_Depression'].replace(2,0)
test_beneficiary['ChronicCond_Diabetes']=test_beneficiary['ChronicCond_Diabetes'].replace(2,0)
test_beneficiary['ChronicCond_Heartfailure']=test_beneficiary['ChronicCond_Heartfailure'].replace(2,0)
test_beneficiary['ChronicCond_IschemicHeart']=test_beneficiary['ChronicCond_IschemicHeart'].replace(2,0)
test_beneficiary['ChronicCond_KidneyDisease']=test_beneficiary['ChronicCond_KidneyDisease'].replace(2,0)
test_beneficiary['ChronicCond_ObstrPulmonary']=test_beneficiary['ChronicCond_ObstrPulmonary'].replace(2,0)
test_beneficiary['ChronicCond_Osteoporasis']=test_beneficiary['ChronicCond_Osteoporasis'].replace(2,0)
test_beneficiary['ChronicCond_rheumatoidarthritis']=test_beneficiary['ChronicCond_rheumatoidarthritis'].replace(2,0)
test_beneficiary['ChronicCond_stroke']=test_beneficiary['ChronicCond_stroke'].replace(2,0)


train_beneficiary['ChronicCond_Alzheimer'].unique()

"""race"""

plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.countplot(data=train_beneficiary,x='Race') # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of Race', fontsize=20)
plt.xlabel('Race', size = 14)
plt.ylabel('Count', size = 14)
count=train_beneficiary['Gender'].value_counts()
plt.show()

"""### Observation 
1. maximum of beneficary belongs to race 1 followed by race 2. 
2. beneficary belongs to race 5 are very less.

country
"""

# county distribution
plt.figure(figsize=(16,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.countplot(data=train_beneficiary,x='State') # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of Country', fontsize=20)
plt.xlabel('State', size = 14)
plt.ylabel('Count', size = 14)
count=train_beneficiary['Gender'].value_counts()
plt.show()

"""### Observations
1. maximum beneficiary come from state code 5.
2. very less beneficiary come from state code 2.

##### continues featues

### IPAnnualReimbursementAmt
"""

plt.figure(figsize=(16,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.distplot(train_beneficiary['IPAnnualReimbursementAmt']) # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of IPAnnualReimbursementAmt', fontsize=20)
plt.xlabel('IPAnnualReimbursementAmt', size = 14)
plt.ylabel('Density', size = 14)
plt.show()

"""### Observation
1. range on annal Reimberusemt of inpaitend data is between 0 to 150000
2. most of the paitent got zero 'IPAnnualReimbursementAmt'
3. very few got reimbursemt between 25000 to 1500000
4. some amount of annualReimbersuemt is very high 
"""

for i in range(0,101,10):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["IPAnnualReimbursementAmt"],i),))

for i in range(90,101,1):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["IPAnnualReimbursementAmt"],i),))

plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.boxplot(y=train_beneficiary['IPAnnualReimbursementAmt']) # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of IPAnnualReimbursementAmt', fontsize=20)
plt.xlabel('IPAnnualReimbursementAmt', size = 14)
plt.ylabel('Percentile', size = 14)
plt.show()

# IQR
for i in range(0,100,25):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["IPAnnualReimbursementAmt"],i)))

"""### Observations
1. diffrence between 99th percentile and 100th percentile is very big.
2. 80% of benefirary got reimbursent less then equal to 5000

##### checking of outliers  in annualReimbursement
"""

for i in np.arange(99,100.1,0.1):
    print('persentle {0} is {1}'.format(i,np.percentile(np.absolute(train_beneficiary["IPAnnualReimbursementAmt"]),i)))

"""1. AS we can see 100 percentile is double of 99.9 percentlile. from this we can conclued that it may be an outlier."""



"""### IPAnnualDeductibleAmt"""

plt.figure(figsize=(16,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.distplot(train_beneficiary['IPAnnualDeductibleAmt'])  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of IPAnnualDeductibleAmt', fontsize=20)
plt.xlabel('IPAnnualDeductibleAmt', size = 14)
plt.ylabel('Density', size = 14)
plt.show()

"""### Observations
1. most of the beneficary paid less ammout to the provier
2. maximum ammout paid to the provier is 40000
"""

for i in range(0,101,10):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["IPAnnualDeductibleAmt"],i),))

for i in range(90,101,1):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["IPAnnualDeductibleAmt"],i),))

plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.boxplot(y=train_beneficiary['IPAnnualDeductibleAmt']) # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of IPAnnualDeductibleAmt', fontsize=20)
plt.xlabel('IPAnnualDeductibleAmt', size = 14)
plt.ylabel('Percentile', size = 14)
plt.show()

# IQR
for i in range(0,100,25):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["IPAnnualDeductibleAmt"],i),))

"""##### checking for outliers"""

for i in np.arange(99,100.1,0.1):
    print('persentle {0} is {1}'.format(i,np.percentile(np.absolute(train_beneficiary["IPAnnualDeductibleAmt"]),i)))

"""### Observations
1. maximum amount paid to provier is 38272.0
2. 75 % or  less paid ammount less then 1068
3. we can see that 100 percentile is very large then 99.9 percentile. it may be an outlier.

### OPAnnualReimbursementAmt
"""

plt.figure(figsize=(16,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.distplot(train_beneficiary['OPAnnualReimbursementAmt']) # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of 0PAnnualReimbursementAmt', fontsize=20)
plt.xlabel('OPAnnualReimbursementAmt', size = 14)
plt.ylabel('Density', size = 14)
plt.show()

"""### Observations
1. maximum amount reimbusemented is 100000
2. most of the benefircay got reimbusment in range of 0 to 10000
"""

for i in range(0,101,10):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["OPAnnualReimbursementAmt"],i),))

for i in range(90,101,1):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["OPAnnualReimbursementAmt"],i),))

plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.boxplot(y=train_beneficiary['OPAnnualReimbursementAmt']) # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of OPAnnualReimbursementAmt', fontsize=20)
plt.xlabel('OPAnnualReimbursementAmt', size = 14)
plt.ylabel('Percentile', size = 14)
plt.show()

for i in range(0,101,25):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["OPAnnualReimbursementAmt"],i),))

# chekcing for outliers
for i in np.arange(99,100.1,0.1):
    print('persentle {0} is {1}'.format(i,np.percentile(np.absolute(train_beneficiary["OPAnnualReimbursementAmt"]),i)))

"""### Observations
1. the max reimbusement amount is 102960
2. 75 % of the beneficary got 1500 or less
3. we can see that 100 percentile is very large then 99.9 percentile. it may be an outlier.
"""



"""### OPAnnualDeductibleAmt"""

plt.figure(figsize=(16,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.distplot(train_beneficiary['OPAnnualDeductibleAmt'])  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of OPAnnualDeductibleAmt', fontsize=20)
plt.xlabel('OPAnnualDeductibleAmt', size = 14)
plt.ylabel('Density', size = 14)
plt.show()

"""### observations
1. most beneficary paid provider ammount in range of 0 to 2000
2. maximim ammount paid to hospital is 14000
"""

for i in range(0,101,10):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["OPAnnualDeductibleAmt"],i),))

for i in range(90,101,1):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["OPAnnualDeductibleAmt"],i),))

plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.boxplot(y=train_beneficiary['OPAnnualDeductibleAmt']) # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of OPAnnualDeductibleAmt', fontsize=20)
plt.xlabel('OPAnnualDeductibleAmt', size = 14)
plt.ylabel('Percentile', size = 14)
plt.show()

for i in range(0,101,25):
    print('persentle {0} is {1}'.format(i,np.percentile(train_beneficiary["OPAnnualDeductibleAmt"],i),))

# chekcing for outliers
for i in np.arange(99,100.1,0.1):
    print('persentle {0} is {1}'.format(i,np.percentile(np.absolute(train_beneficiary["OPAnnualDeductibleAmt"]),i)))

"""###  Observations
1. max amout paid by the beneficary is 13840
2. 75 % or less paid amount 460
3. we can see that 100 percentile is very large then 99.9 percentile. it may be an outlier.

#### checking correlation between beneficary data (person)
"""

plt.figure(figsize=(30,20))
corrMatrix = train_beneficiary.corr()
hm = sns.heatmap(corrMatrix, annot = True)
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12)
plt.show()



"""#### checking correlation between beneficary data (spearman)"""

plt.figure(figsize=(30,20))
corrMatrix = train_beneficiary.corr(method='spearman')
hm = sns.heatmap(corrMatrix, annot = True)
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12)
plt.show()

"""### observations

1. IPAnnualDeductibleAmt is very collinear  with IPAnnualReimbursementAmt with 0.97
2. OPAnnualReimbursementAmt is collinear with OPAnnualDeductibleAmt with 0.66

we can remove one of the feature IPAnnualDeductibleAmt or IPAnnualReimbursementAmt
"""

plt.figure(figsize=(12,6))
sns.scatterplot(x='IPAnnualReimbursementAmt',y='IPAnnualDeductibleAmt',data=train_beneficiary)
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('IPAnnualReimbursementAmt vs IPAnnualDeductibleAmt', fontsize=20)
plt.xlabel('IPAnnualReimbursementAmt', size = 14)
plt.ylabel('IPAnnualDeductibleAmt', size = 14)
plt.show()

"""## Outpaitent data"""

train_outpatient.head()

train_outpatient.info()

"""#### InscClaimAmtReimbursed"""

train_outpatient['InscClaimAmtReimbursed'].isnull().any()

plt.figure(figsize=(16,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.distplot(train_outpatient['InscClaimAmtReimbursed'])  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of InscClaimAmtReimbursed', fontsize=20)
plt.xlabel('InscClaimAmtReimbursed', size = 14)
plt.ylabel('Density', size = 14)
plt.show()

for i in range(0,101,25):
    print('persentle {0} is {1}'.format(i,np.percentile(train_outpatient["InscClaimAmtReimbursed"],i),))

for i in range(0,101,10):
    print('persentle {0} is {1}'.format(i,np.percentile(train_outpatient["InscClaimAmtReimbursed"],i),))

for i in range(90,101,1):
    print('persentle {0} is {1}'.format(i,np.percentile(train_outpatient["InscClaimAmtReimbursed"],i),))

for i in np.arange(99,100.1,0.1):
    print('persentle {0} is {1}'.format(i,np.percentile(np.absolute(train_outpatient["InscClaimAmtReimbursed"]),i)))

plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.boxplot(y=train_outpatient['InscClaimAmtReimbursed']) # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of InscClaimAmtReimbursed', fontsize=20)
plt.xlabel('InscClaimAmtReimbursed', size = 14)
plt.ylabel('Percentile', size = 14)
plt.show()

"""### Observations
1. max amount filed for reimbusement is 102500
1. 75 % of claim field for amount 200
3. There may be some outliers in our data.

### Attending physican
"""

# There are some data poins which are na becasue of that they are not attended by physicain or some hunam error.

train_outpatient['AttendingPhysician'].isnull().any()
train_outpatient['AttendingPhysician']=train_outpatient['AttendingPhysician'].fillna(0)
train_outpatient['AttendingPhysician'].isnull().any()
# filling nan values in test data
test_outpatient['AttendingPhysician']=test_outpatient['AttendingPhysician'].fillna(0)

# county distribution
plt.figure(figsize=(20,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.countplot(data=train_outpatient,x='AttendingPhysician',order=train_outpatient['AttendingPhysician'].value_counts()[:20].index) # name of the category(index)  
plt.xticks(size = 10) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of AttendingPhysician', fontsize=20)
plt.xlabel('AttendingPhysician', size = 14)
plt.ylabel('Count', size = 14)
plt.show()

train_outpatient['AttendingPhysician'].value_counts()[:20].index

"""### Observation
1. PHY330576 has attended maximum number on oud patients.
2. 1300 aroud patients that were not attended by the physician.

### Operating physician
"""

# checking nan values 
# filling them with zero 
train_outpatient['OperatingPhysician'].isnull().any()
train_outpatient['OperatingPhysician']=train_outpatient['OperatingPhysician'].fillna(0)
train_outpatient['OperatingPhysician'].isnull().any()
# removing na from test data
test_outpatient['OperatingPhysician']=test_outpatient['OperatingPhysician'].fillna(0)

#counting the physican code 
operating_phy=train_outpatient['OperatingPhysician'].value_counts()[1:21]

# county distribution
plt.figure(figsize=(20,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
# sns.barplot(operating_phy.index,operating_phy.values,order=operating_phy.index) # name of the category(index)  

df = pd.DataFrame({'OperatingPhysician': operating_phy.index, 'Count': operating_phy.values})
sns.barplot(data=df, x='OperatingPhysician', y='Count', order=operating_phy.index) #newly added


plt.xticks(size = 10) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of OperatingPhysician', fontsize=20)
plt.xlabel('OperatingPhysician', size = 14)
plt.ylabel('Count', size = 14)
plt.show()

train_outpatient['OperatingPhysician'].value_counts()[:10]

"""### Observation
1. There are more then 400000 patients who were not attented by the  any of the operating physican.

### Other physician
"""

# checking nan values 
# filling them with zero 
train_outpatient['OtherPhysician'].isnull().any()
train_outpatient['OtherPhysician']=train_outpatient['OtherPhysician'].fillna(0)
# remvoing nan with 0 in test otherphysican
test_outpatient['OtherPhysician']=test_outpatient['OtherPhysician'].fillna(0)

test_outpatient['OtherPhysician'].value_counts()

"""### claim diagnose code"""

len(train_outpatient['ClmDiagnosisCode_1'].unique())
# there are 10355 unique claimDaignosiscode

## replacing nan values with zero
train_outpatient['ClmDiagnosisCode_1'].isnull().any()
train_outpatient['ClmDiagnosisCode_1']=train_outpatient['ClmDiagnosisCode_1'].fillna(0)
train_outpatient['ClmDiagnosisCode_2']=train_outpatient['ClmDiagnosisCode_2'].fillna(0)
train_outpatient['ClmDiagnosisCode_3']=train_outpatient['ClmDiagnosisCode_3'].fillna(0)
train_outpatient['ClmDiagnosisCode_4']=train_outpatient['ClmDiagnosisCode_4'].fillna(0)
train_outpatient['ClmDiagnosisCode_5']=train_outpatient['ClmDiagnosisCode_5'].fillna(0)
train_outpatient['ClmDiagnosisCode_6']=train_outpatient['ClmDiagnosisCode_6'].fillna(0)
train_outpatient['ClmDiagnosisCode_7']=train_outpatient['ClmDiagnosisCode_7'].fillna(0)
train_outpatient['ClmDiagnosisCode_8']=train_outpatient['ClmDiagnosisCode_8'].fillna(0)
train_outpatient['ClmDiagnosisCode_9']=train_outpatient['ClmDiagnosisCode_9'].fillna(0)
train_outpatient['ClmDiagnosisCode_10']=train_outpatient['ClmDiagnosisCode_10'].fillna(0)
train_outpatient['ClmDiagnosisCode_1'].isnull().any()
# replacing nan valuse in test data
test_outpatient['ClmDiagnosisCode_1']=test_outpatient['ClmDiagnosisCode_1'].fillna(0)
test_outpatient['ClmDiagnosisCode_2']=test_outpatient['ClmDiagnosisCode_2'].fillna(0)
test_outpatient['ClmDiagnosisCode_3']=test_outpatient['ClmDiagnosisCode_3'].fillna(0)
test_outpatient['ClmDiagnosisCode_4']=test_outpatient['ClmDiagnosisCode_4'].fillna(0)
test_outpatient['ClmDiagnosisCode_5']=test_outpatient['ClmDiagnosisCode_5'].fillna(0)
test_outpatient['ClmDiagnosisCode_6']=test_outpatient['ClmDiagnosisCode_6'].fillna(0)
test_outpatient['ClmDiagnosisCode_7']=test_outpatient['ClmDiagnosisCode_7'].fillna(0)
test_outpatient['ClmDiagnosisCode_8']=test_outpatient['ClmDiagnosisCode_8'].fillna(0)
test_outpatient['ClmDiagnosisCode_9']=test_outpatient['ClmDiagnosisCode_9'].fillna(0)
test_outpatient['ClmDiagnosisCode_10']=test_outpatient['ClmDiagnosisCode_10'].fillna(0)

"""### claim procedure code"""

train_outpatient['ClmProcedureCode_1'].isnull().any()
## remving nan valeus form the claimprocedure
train_outpatient['ClmProcedureCode_1']=train_outpatient['ClmProcedureCode_1'].fillna(0)
train_outpatient['ClmProcedureCode_2']=train_outpatient['ClmProcedureCode_2'].fillna(0)
train_outpatient['ClmProcedureCode_3']=train_outpatient['ClmProcedureCode_3'].fillna(0)
train_outpatient['ClmProcedureCode_4']=train_outpatient['ClmProcedureCode_4'].fillna(0)
train_outpatient['ClmProcedureCode_5']=train_outpatient['ClmProcedureCode_5'].fillna(0)
train_outpatient['ClmProcedureCode_6']=train_outpatient['ClmProcedureCode_6'].fillna(0)
## removing nan values from test data
test_outpatient['ClmProcedureCode_1']=test_outpatient['ClmProcedureCode_1'].fillna(0)
test_outpatient['ClmProcedureCode_2']=test_outpatient['ClmProcedureCode_2'].fillna(0)
test_outpatient['ClmProcedureCode_3']=test_outpatient['ClmProcedureCode_3'].fillna(0)
test_outpatient['ClmProcedureCode_4']=test_outpatient['ClmProcedureCode_4'].fillna(0)
test_outpatient['ClmProcedureCode_5']=test_outpatient['ClmProcedureCode_5'].fillna(0)
test_outpatient['ClmProcedureCode_6']=test_outpatient['ClmProcedureCode_6'].fillna(0)

"""### DeductibleAmtPaid"""

train_outpatient['DeductibleAmtPaid'].isnull().any()

plt.figure(figsize=(16,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.distplot(train_outpatient['DeductibleAmtPaid'])  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of DeductibleAmtPaid', fontsize=20)
plt.xlabel('DeductibleAmtPaid', size = 14)
plt.ylabel('Density', size = 14)
plt.show()

for i in range(0,101,25):
    print('persentle {0} is {1}'.format(i,np.percentile(train_outpatient["DeductibleAmtPaid"],i),))

for i in range(90,101,1):
    print('persentle {0} is {1}'.format(i,np.percentile(train_outpatient["DeductibleAmtPaid"],i),))

for i in np.arange(99,100.1,0.1):
    print('persentle {0} is {1}'.format(i,np.percentile(np.absolute(train_outpatient["DeductibleAmtPaid"]),i)))

plt.figure(figsize=(7,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.boxplot(y=train_outpatient['DeductibleAmtPaid']) # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of DeductibleAmtPaid', fontsize=20)
plt.xlabel('DeductibleAmtPaid', size = 14)
plt.ylabel('Percentile', size = 14)
plt.show()



"""### Observations
1. maximum deductiableamtpaid is 897
2. 75% of patients paid zero(0) deductible amount.
3. There are some outliers in DeductibleAmtPaid

### claim admit diagnose code
"""

train_outpatient['ClmAdmitDiagnosisCode'].isnull().any()
# repaling nan values with 0.
train_outpatient['ClmAdmitDiagnosisCode']=train_outpatient['ClmAdmitDiagnosisCode'].fillna(0)
# removing from test data
test_outpatient['ClmAdmitDiagnosisCode']=test_outpatient['ClmAdmitDiagnosisCode'].fillna(0)

## top 20 clmAdmitDiagnosisicode
train_outpatient['ClmAdmitDiagnosisCode'].value_counts()[:20].index

train_outpatient.isna().sum()

#https://datatofish.com/correlation-matrix-pandas/#:~:text=Steps%20to%20Create%20a%20Correlation%20Matrix%20using%20Pandas,above%20dataset%20in%20Python%3A%20import...%20Step%203%3A%20
plt.figure(figsize=(15,8))
corrMatrix = train_outpatient.corr()
hm = sns.heatmap(corrMatrix, annot = True)
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12)
plt.show()

"""### Observations
1. There are lots of collinear featues if outpatient data

  a) InscClaimAmtReimbursed with ClmProcedureCode_4

  b) ClmProcedureCode_4 with ClmProcedureCode_2

we could remove clmProcedureCode_4 in final data

### INPatient data
All the fearues are common to outpaitinet data except 3 columns
"""

for i in train_inpatient.columns:
    if i not in train_outpatient.columns:
        print(i)

"""### DiagnosisGroupCode"""

train_inpatient.DiagnosisGroupCode.isnull().any()

dig_count=train_inpatient['DiagnosisGroupCode'].value_counts()[:30]

# county distribution
plt.figure(figsize=(20,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 

#sns.barplot(dig_count.index,dig_count.values,order=dig_count.index) # name of the category(index)  

df = pd.DataFrame({'Category': dig_count.index, 'Count': dig_count.values})
sns.barplot(data=df, x='Category', y='Count', order=dig_count.index)


# plt.xticks(size = 10) # size of x axis indicators(yes/no)
# plt.yticks(size = 12) 
# plt.title('Distribution of DiagnosisGroupCode', fontsize=20)
# plt.xlabel('DiagnosisGroupCode', size = 14)
# plt.ylabel('Count', size = 14)
# plt.show()

"""### observations  
1. patinet admitted in the hospital maximum of them are diagonsed with 882.
2. all code have same frequency in the data.

### Creating New Featues from data.(Features Engineering)

#### 1. Beneficary data.
"""

train_beneficiary.columns

"""we have almost all values in DOD(date of death) is NaN by this we can add another column weather a dead or not.
if value is Nan this means he is alive(0) or has a date means he is dead(1).

#### Dead_or_Alive
"""

#https://www.kaggle.com/rahuly93/medicare-provider-fraud-detection


train_beneficiary.loc[train_beneficiary.DOD.isna(),'Dead_or_Alive']=0
train_beneficiary.loc[train_beneficiary.DOD.notna(),'Dead_or_Alive']=1
# test data

test_beneficiary.loc[test_beneficiary.DOD.isna(),"Dead_or_Alive"]=0
test_beneficiary.loc[test_beneficiary.DOD.notna(),"Dead_or_Alive"]=1

"""##### Age

we will calculate age from the latest date present in our data.
"""

train_beneficiary.DOD.unique()

#https://datatofish.com/strings-to-datetime-pandas/
train_beneficiary['DOB']=pd.to_datetime(train_beneficiary['DOB'],format='%Y-%m-%d')
train_beneficiary['DOD']=pd.to_datetime(train_beneficiary['DOD'],format='%Y-%m-%d')
# test
test_beneficiary['DOD']=pd.to_datetime(test_beneficiary['DOD'],format='%Y-%m-%d')
test_beneficiary['DOB']=pd.to_datetime(test_beneficiary['DOB'],format='%Y-%m-%d')

# subracting dod form dob to get the age accoring to it.
train_beneficiary['Age']= round((train_beneficiary['DOD']-train_beneficiary['DOB']).dt.days/365)
#testB
test_beneficiary['Age']=round((test_beneficiary['DOD']-test_beneficiary['DOB']).dt.days/365)

train_beneficiary.Age.unique()
test_beneficiary.Age.unique()

# fill the nan vlaues accorindig to the latest date presnt in DoD.
train_beneficiary['Age']=train_beneficiary['Age'].fillna(round((pd.to_datetime('2009-12-01',format='%Y-%m-%d')-train_beneficiary['DOB']).dt.days/365))

# test
test_beneficiary['Age']=test_beneficiary['Age'].fillna(round((pd.to_datetime('2009-12-01',format='%Y-%m-%d')-test_beneficiary['DOB']).dt.days/365))
# cheking
train_beneficiary['Age'].isna().any()
test_beneficiary['Age'].isna().any()

age_count=train_beneficiary.Age.value_counts()
plt.figure(figsize=(30,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 

df=pd.DataFrame({"Category" : age_count.index, "Count" : age_count.values})
sns.barplot(data=df,x="Category",y="Count",order=age_count.index) # name of the category(index)  


plt.xticks(size = 10) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of Age', fontsize=20)
plt.xlabel('Age', size = 14)
plt.ylabel('Count', size = 14)
plt.show()

"""###  Observations
1. maximum patinet belongs to age group of 80 to 50
2. very few paitnets belongs to age group of 30 or less.

##### Total_chornic_condition.

1. we have 10 diffrent chornic conditions for a paitent we can sum all these chronic conditions to get how many chornic disease have a single paitent
"""

train_beneficiary['Tolat_chronic_cond']=  (train_beneficiary['ChronicCond_Alzheimer'] + train_beneficiary['ChronicCond_Cancer'] +
                                          train_beneficiary['ChronicCond_Depression'] + train_beneficiary['ChronicCond_Diabetes'] +
                                          train_beneficiary['ChronicCond_Heartfailure'] + train_beneficiary['ChronicCond_IschemicHeart'] +
                                          train_beneficiary['ChronicCond_KidneyDisease'] + train_beneficiary['ChronicCond_ObstrPulmonary'] +
                                           train_beneficiary['ChronicCond_rheumatoidarthritis'] + train_beneficiary['ChronicCond_stroke']
                                          )
# test 
test_beneficiary['Tolat_chronic_cond']=  (test_beneficiary['ChronicCond_Alzheimer'] + test_beneficiary['ChronicCond_Cancer'] +
                                          test_beneficiary['ChronicCond_Depression'] + test_beneficiary['ChronicCond_Diabetes'] +
                                          test_beneficiary['ChronicCond_Heartfailure'] + test_beneficiary['ChronicCond_IschemicHeart'] +
                                          test_beneficiary['ChronicCond_KidneyDisease'] + test_beneficiary['ChronicCond_ObstrPulmonary'] +
                                           test_beneficiary['ChronicCond_rheumatoidarthritis'] + test_beneficiary['ChronicCond_stroke']
                                          )

plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.countplot(data=train_beneficiary,x='Tolat_chronic_cond') # conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution of total_chornic_cond', fontsize=20)
plt.xlabel('tolal_chronic_condi', size = 14)
plt.ylabel('Density', size = 14)
plt.show()

"""### Observations
1. most of patients have 1-6 chronic disease
2. very few of them have all 10 chronic disease.

#### cheking collinearity with new featues
"""



plt.figure(figsize=(30,20))
corrMatrix = train_beneficiary.corr(method='spearman')
hm = sns.heatmap(corrMatrix, annot = True)
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12)
plt.show()

"""### Observations
1. new featues are not highly colliner with other featues

### INPatient data
"""

train_inpatient.columns

"""##### Admitted_or_Not

Adding a new column for weather a patient is admitted or not  for inpaitent data we will indicate it with 1.
"""

#Traindata
train_inpatient['Admitted_or_Not']=1
# test_data
test_inpatient['Admitted_or_Not']=1

"""### Outpatient"""

train_outpatient.columns

"""##### Admitted_or_Not

Adding a new column for weather a patient is admitted or not  for outpatient data we will indicate it with 0.
"""

#Traindata
train_outpatient['Admitted_or_Not']=0
# test_data
test_outpatient['Admitted_or_Not']=0







"""## Merging data

1. first we merge inpatient data and outpatient by common columns present in both the data frame.
2. second we  merge the merged data set in first step with beneficiary data by BeneId (Beneficiary id).
3. Third we merge the train lables will above merged data set by Provider id.

### 1. Merging Inpatient and Outpatient data
"""

print(train_inpatient.shape,train_outpatient.shape)
# print(test_inpatient.shape,test_outpatient.shape)

# getting commonn columns
comm_col=[]
for i in train_inpatient.columns:
    if i in train_outpatient.columns:
        comm_col.append(i)
        print(i)
len(comm_col)
# comm_col=[]
# for i in test_inpatient.columns:
#     if i in test_outpatient.columns:
#         comm_col.append(i)
#         print(i)
# len(comm_col)

IN_OUT_train=pd.merge(train_inpatient,train_outpatient,left_on=comm_col,right_on=comm_col,how='outer')
IN_OUT_test=pd.merge(test_inpatient,test_outpatient,how='outer')
IN_OUT_train.columns
IN_OUT_test.columns

"""###### Admitted_days

we will get admitted_days by subtraching DiscargeDt to AdmissinonDt.
"""

IN_OUT_train['AdmissionDt'].isna().any()
# IN_OUT_train['DischargeDt'].isna().sum()
# IN_OUT_test['AdmissionDt_x'].isna().any()

IN_OUT_train['AdmissionDt']=pd.to_datetime(IN_OUT_train['AdmissionDt'],format='%Y-%m-%d')
IN_OUT_train['DischargeDt']=pd.to_datetime(IN_OUT_train['DischargeDt'],format='%Y-%m-%d')
#test
IN_OUT_test['AdmissionDt']=pd.to_datetime(IN_OUT_test['AdmissionDt'],format='%Y-%m-%d')
IN_OUT_test['DischargeDt']=pd.to_datetime(IN_OUT_test['DischargeDt'],format='%Y-%m-%d')

IN_OUT_train['Admitted_days']=round((IN_OUT_train['DischargeDt']-IN_OUT_train['AdmissionDt']).dt.days)
#test
IN_OUT_test['Admitted_days']=round((IN_OUT_test['DischargeDt']-IN_OUT_test['AdmissionDt']).dt.days)

#filling nan values with 1 because patient is admitted for mininmun day is 1.
IN_OUT_train['Admitted_days']=IN_OUT_train['Admitted_days'].fillna(1)
IN_OUT_test['Admitted_days']=IN_OUT_test['Admitted_days'].fillna(1)
IN_OUT_train['Admitted_days'].isna().any()

"""##### Claim_time

we will caluculate claim_time by subtracting claim_start_date by claim_end_date.
"""

IN_OUT_train['ClaimStartDt'].isna().any()
IN_OUT_train['ClaimEndDt'].isna().any()

IN_OUT_train['ClaimStartDt']=pd.to_datetime(IN_OUT_train['ClaimStartDt'],format='%Y-%m-%d')
IN_OUT_train['ClaimEndDt']=pd.to_datetime(IN_OUT_train['ClaimEndDt'],format='%Y-%m-%d')
#test
IN_OUT_test['ClaimStartDt']=pd.to_datetime(IN_OUT_test['ClaimStartDt'],format='%Y-%m-%d')
IN_OUT_test['ClaimEndDt']=pd.to_datetime(IN_OUT_test['ClaimEndDt'],format='%Y-%m-%d')

IN_OUT_train['Claim_time']=round((IN_OUT_train['ClaimEndDt']-IN_OUT_train['ClaimStartDt']).dt.days)+1 # adding one becase atleast 1 day to process the claim
#test
IN_OUT_test['Claim_time']=round((IN_OUT_test['ClaimEndDt']-IN_OUT_test['ClaimStartDt']).dt.days)+1



"""###### Amount_get

we will calulate amount patinet will get.

"""

IN_OUT_train['InscClaimAmtReimbursed'].isna().any()
IN_OUT_train['DeductibleAmtPaid'].isna().any()

IN_OUT_train['DeductibleAmtPaid']=IN_OUT_train['DeductibleAmtPaid'].fillna(0)
#test
IN_OUT_test['DeductibleAmtPaid']=IN_OUT_test['DeductibleAmtPaid'].fillna(0)

IN_OUT_train['Amount_get']=IN_OUT_train['InscClaimAmtReimbursed']-IN_OUT_train['DeductibleAmtPaid']
#test
IN_OUT_test['Amount_get']=IN_OUT_test['InscClaimAmtReimbursed']-IN_OUT_test['DeductibleAmtPaid']

train_inpatient.ClmDiagnosisCode_4

IN_OUT_train=IN_OUT_train.fillna(0)





"""#### checking collinearity on merged data """

plt.figure(figsize=(30,20))
corrMatrix = IN_OUT_train.corr(method='spearman')
hm = sns.heatmap(corrMatrix, annot = True)
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12)
plt.show()

"""#### Observations

1. InscClaimAmtReimbussed is collinear with Amout_get
2. Admitted_days is collinear with Admitted_or_not

### 2. Merging merged inpatient and outpatient data with beneficiary data
"""

in_out_bene_train=pd.merge(IN_OUT_train,train_beneficiary,on='BeneID',how='inner')
#test data
in_out_bene_test=pd.merge(IN_OUT_test,test_beneficiary,on="BeneID",how='inner')

in_out_bene_train.head()
print(in_out_bene_train.shape,in_out_bene_test.shape)

# #checking null valeus in all  columns
# for i in in_out_bene_train.columns:
#     print(i, "=" , in_out_bene_train[i].isna().any())

"""### 3.  Merging in_out_bene data with train_labels data."""

final_data_train=pd.merge(in_out_bene_train,train_label,on='Provider',how='inner')
final_data_test=pd.merge(in_out_bene_test,test_label,on='Provider',how='inner')
print(final_data_train.shape,final_data_test.shape)

final_data_test.columns

"""##### total_ip_op_amount_reimb

calculaing the total inpatient,outpatient annual reimbursement amount. 
"""

final_data_train['Total_ip_op_amount_reimb']=final_data_train['IPAnnualReimbursementAmt']+final_data_train['OPAnnualReimbursementAmt']
# test
final_data_test['Total_ip_op_amount_reimb']=final_data_test['IPAnnualReimbursementAmt']+final_data_test['OPAnnualReimbursementAmt']

"""###### total_ip_op_amount_deduct

calculating the total inpatient, outpatient annual deductible amount.
"""

final_data_train['total_ip_op_amount_deduct']=final_data_train['OPAnnualDeductibleAmt']+final_data_train['IPAnnualDeductibleAmt']
# test
final_data_test['total_ip_op_amount_deduct']=final_data_test['OPAnnualDeductibleAmt']+final_data_test['IPAnnualDeductibleAmt']

final_data_train.head()

#checking null valeus in all  columns
for i in final_data_train.columns:
    print(i, "=" , final_data_train[i].isna().any())

"""#### handling missing values 

1. AttendingPhysician, OperatingPhysician, OtherPhysician having Nan values this is becauese of a perticular beneficicary or patient have not attended by the Physicain we can fill these missing values by **0**

2. AdmissionDt and discharegedt has Nan values for outpatient so we can fill these values by **0**

3. Claim Diagnose code and Claim procedure code can have Nan values becaseue a perticular code may not be applied on a patient so we can fill Nan values with **0**

4. DOD (date of death) is not applicable for alive patients so we can fill these valeus by **0**

"""

final_data_train=final_data_train.fillna(0)
# test
final_data_test=final_data_test.fillna(0)

print(final_data_train.isna().any().tolist())

"""## EDA  on final Merged data"""

## race belong to which class
plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud", height=5) \
   .map(sns.countplot, "Race").add_legend()# conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Race_belong_to_potentialFraud', fontsize=20)
plt.xlabel('Race', size = 14)
plt.ylabel('Count', size = 14)
plt.show()

"""### Observations
1. Almost all from race 5 and 3 belongs to PotentialFraud.
2. There is 50-50 chances that if provider come form race 1 belongs to Fraud or not.
"""

# distributionn of age.
plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud",height=10) \
   .map(sns.distplot, "Age").add_legend()# conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Age_belong_to_potentialFraud', fontsize=20)
plt.xlabel('Age', size = 14)
plt.ylabel('denisty', size = 14)
plt.show()

"""### Observations
1. There is overlaping we can not distingush by the Age of Beneficary.
"""

# # distributionn of total_chornic_condtion.
# plt.figure(figsize=(12,6)) # hight and width of plot 
# sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud",height=6) \
   .map(sns.distplot, "Tolat_chronic_cond").add_legend()# conting unique values  
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Total_chronic_condition_belong_to_potentialFraud', fontsize=20)
plt.xlabel('Total_chronic_condtion', size = 14)
plt.ylabel('denisty', size = 14)
plt.show()

"""### Observations
1. There are overlaping in pdf of total_chronic_condition
"""

# age vs no of admitted days.
plt.figure(figsize=(12,6))
sns.lineplot(x='Age',y='Admitted_days',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Age vs Admitted_days', fontsize=20)
plt.xlabel('Age', size = 14)
plt.ylabel('Admitted_days', size = 14)
plt.show()

"""### Observations
1. The fraud provider have no. of admitted_days are more.
2. The number of admitted days is more for age groud between 25-45. we can see a sharp increase.
3. Age and admitted_days may do well while detecting potential fraud.

"""

# displot of InscClaimAmtReimbursed
plt.figure(figsize=(12,6))
sns.set_style('whitegrid')
sns.displot(data=final_data_train, x="InscClaimAmtReimbursed", hue="PotentialFraud", kind="kde")
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution_Of_InscClaimAmtReimbursed', fontsize=20)
plt.xlabel('InscClaimAmtReimbursed', size = 14)
plt.ylabel('denisty', size = 14)
plt.show()

"""  ### Observations
  1. There is overlaping in InsClaimAmtReibursed.
"""

#CDF_Of_InscClaimAmtReimbursed
plt.figure(figsize=(12,6))
sns.displot(data=final_data_train, x="InscClaimAmtReimbursed", hue="PotentialFraud", kind="ecdf")
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('CDF_Of_InscClaimAmtReimbursed', fontsize=20)
plt.xlabel('InscClaimAmtReimbursed', size = 14)
plt.ylabel('denisty', size = 14)
plt.show()

"""### Observations
1. The cdf of inscClamAmt Remibused is also overlaping.
"""

#Displot_Of_DeductibleAmtPaid
plt.figure(figsize=(12,6))
sns.set_style('whitegrid')
sns.displot(data=final_data_train, x="DeductibleAmtPaid", hue="PotentialFraud", kind="hist",kde=True)
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Distribution_Of_DeductibleAmtPaid', fontsize=20)
plt.xlabel('DeductibleAmtPaid', size = 14)
plt.ylabel('Count', size = 14)
plt.show()

sns.displot(data=final_data_train, x="DeductibleAmtPaid", hue="PotentialFraud", kind="ecdf")
plt.title('CDF_Of_DeductibleAmtPaid', fontsize=20)
plt.xlabel('DeductibleAmtPaid', size = 14)
plt.ylabel('Proportion', size = 14)

"""### Observations.
1. There is overlapping in distributions of DeductibleAmtPaid.
2. CDF is also showing same behaviour.
"""

plt.figure(figsize=(12,6))
sns.scatterplot(x='InscClaimAmtReimbursed',y='DeductibleAmtPaid',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('InscClaimAmtReimbursed vs DeductibleAmtPaid', fontsize=20)
plt.xlabel('InscClaimAmtReimbursed', size = 14)
plt.ylabel('DeductibleAmtPaid', size = 14)
plt.show()

"""### Observations
1. The Potential_frauds have high inscClaimAmtReimbu.\
2. some of the Deductible amont is zero and claim amont is high.

"""

plt.figure(figsize=(12,6))
sns.lineplot(x='Claim_time',y='Admitted_days',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Claim_time vs Admitted_days', fontsize=20)
plt.xlabel('Claim_time', size = 14)
plt.ylabel('Admitted_days', size = 14)
plt.show()

"""### Observations
1. There is Overlaping in claim_time and Admitted_day we can not conclude any thing from adimitted days and claim days

"""

plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud",height=10) \
   .map(sns.distplot, "IPAnnualReimbursementAmt").add_legend()# conting unique values  

plt.title('Distribution_Of_IPAnnualReimbursementAmt', fontsize=20)
plt.xlabel('IPAnnualReimbursementAmt', size = 14)
plt.ylabel('density', size = 14)
plt.show()

# distrubution of IPAnnualDeductibleAmt
plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud",height=10) \
   .map(sns.distplot, "IPAnnualDeductibleAmt").add_legend()# conting unique values  

plt.title('Distribution_Of_IPAnnualDeductibleAmt', fontsize=20)
plt.xlabel('IPAnnualDeductibleAmt', size = 14)
plt.ylabel('density', size = 14)
plt.show()

# scatter plot of IPAnnualDeductibleAmt and IPAnnualReimbursementAmt
plt.figure(figsize=(12,6))
sns.scatterplot(x='IPAnnualDeductibleAmt',y='IPAnnualReimbursementAmt',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('IPAnnualDeductibleAmt vs IPAnnualReimbursementAmt', fontsize=20)
plt.xlabel('IPAnnualDeductibleAmt', size = 14)
plt.ylabel('IPAnnualReimbursementAmt', size = 14)
plt.show()
# final_data_test.IPAnnualReimbursementAmt

"""### Observations
1. The distribution of IPAnnualReimbursementAmt is overlaping.
2. The diribution of IPAnnualDeductibleAmt is also overlaping.
3. There are few points in which deductible amount and Reimbursemnet both are high for fraud
4. Most of the points have low decductable amont and high reimbursement amount.

"""

# distribution of OPAnnualReimbursementAmt
plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud",height=10) \
   .map(sns.distplot, "OPAnnualReimbursementAmt").add_legend()# conting unique values  

plt.title('Distribution_Of_OPAnnualReimbursementAmt', fontsize=20)
plt.xlabel('OPAnnualReimbursementAmt', size = 14)
plt.ylabel('density', size = 14)
plt.show()

"""### Observations
1. Both curvs are overlaping 
"""

# distribution of OPAnnualDeductibleAmt
plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud",height=10) \
   .map(sns.distplot, "OPAnnualDeductibleAmt").add_legend()# conting unique values  

plt.title('Distribution_Of_OPAnnualDeductibleAmt', fontsize=20)
plt.xlabel('OPAnnualDeductibleAmt', size = 14)
plt.ylabel('density', size = 14)
plt.show()

"""### Observations
1. Both curvs are overlaping 
"""

plt.figure(figsize=(12,6))
sns.scatterplot(x='OPAnnualReimbursementAmt',y='OPAnnualDeductibleAmt',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('OPAnnualReimbursementAmt vs OPAnnualDeductibleAmt', fontsize=20)
plt.xlabel('OPAnnualReimbursementAmt', size = 14)
plt.ylabel('OPAnnualDeductibleAmt', size = 14)
plt.show()

"""### Observations
1. OPAnnualReimbursementAmt and OPAnnualDeductibleAmt are dense in x=20000 and y=4000 region
2. most of the points are overlaping.
3. Features are looking collinear
"""

#distribution of total inpaitent outpatient reimbursemnet amount
plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud",height=10) \
   .map(sns.distplot, "Total_ip_op_amount_reimb").add_legend()# conting unique values  

plt.title('Distribution_Of_Total_ip_op_amount_reimb', fontsize=20)
plt.xlabel('Total_ip_op_amount_reimb', size = 14)
plt.ylabel('density', size = 14)
plt.show()

"""### Observations
1. There is overlaping in the distribution
2. most of the total inpatient outpaitent amount lie between 0 to 5000
"""

# distributon of tolatl inpatient outpatient deductable amount.
plt.figure(figsize=(12,6)) # hight and width of plot 
sns.set_style('whitegrid') # backgroud of plot 
sns.FacetGrid(final_data_train, hue="PotentialFraud",height=10) \
   .map(sns.distplot, "total_ip_op_amount_deduct").add_legend()# conting unique values  

plt.title('Distribution_Of_total_ip_op_amount_deduct', fontsize=20)
plt.xlabel('total_ip_op_amount_deduct', size = 14)
plt.ylabel('density', size = 14)
plt.show()

"""### Observation
1. There is overlaping in distribution.
2. Maximum total deduction in amount is 0.
"""

plt.figure(figsize=(12,6))
sns.scatterplot(x='total_ip_op_amount_deduct',y='Total_ip_op_amount_reimb',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('total_ip_op_amount_deduct vs Total_ip_op_amount_reimb', fontsize=20)
plt.xlabel('total_ip_op_amount_deduct', size = 14)
plt.ylabel('Total_ip_op_amount_reimb', size = 14)
plt.show()

"""### Observation
1. most of the points are overlaping.
2. there are some points which have low deduction and high reimbursment amount.
"""

# line plot between chronic condition vs cliam time
plt.figure(figsize=(12,6))
sns.lineplot(x='Claim_time',y='Tolat_chronic_cond',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Claim_time vs Tolat_chronic_cond', fontsize=20)
plt.xlabel('Claim_time', size = 14)
plt.ylabel('Tolat_chronic_cond', size = 14)
plt.show()

"""### Observations
1. claim time is more for more Chronic Condition
2. more then 5 chronic condition has claim time between 25 to 35 days.
"""

# line plot between Amount_get vs cliam time
plt.figure(figsize=(12,6))
sns.lineplot(x='Claim_time',y='Amount_get',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Claim_time vs Amount_get', fontsize=20)
plt.xlabel('Claim_time', size = 14)
plt.ylabel('Amount_get', size = 14)
plt.show()

"""### Oservations
1. For amount less then 10,000 claim time is 0 to 20 days
2. Claim time is more for more Amount_get.

### Checking Collinearity in final merged data set
"""

#https://datatofish.com/correlation-matrix-pandas/#:~:text=Steps%20to%20Create%20a%20Correlation%20Matrix%20using%20Pandas,above%20dataset%20in%20Python%3A%20import...%20Step%203%3A%20
plt.figure(figsize=(30,20))
corrMatrix = final_data_train.corr()
hm = sns.heatmap(corrMatrix, annot = True)

"""#### Observations
1. Admitted days are collinear with deductible amount paid
2. InscClaimAmountReimbursed with Amount get.
3. Total_in_out_anual_reimbrused with IPanualAmtReimbrused.

#### analyis on collinear features.
"""

plt.figure(figsize=(12,6))
sns.lineplot(x='InscClaimAmtReimbursed',y='Amount_get',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('InscClaimAmtReimbursed vs Amount_get', fontsize=20)
plt.xlabel('InscClaimAmtReimbursed', size = 14)
plt.ylabel('Amount_get', size = 14)
plt.show()

"""#### observations 
1. showing linear behaviour

"""

plt.figure(figsize=(12,6))
sns.lineplot(x='Admitted_days',y='DeductibleAmtPaid',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Admitted_days vs DeductibleAmtPaid', fontsize=20)
plt.xlabel('Admitted_days', size = 14)
plt.ylabel('DeductibleAmtPaid', size = 14)
plt.show()

"""#### observation
1. Almost stright line  to x-axis.
 
"""

plt.figure(figsize=(12,6))
sns.scatterplot(x='Total_ip_op_amount_reimb',y='IPAnnualReimbursementAmt',data=final_data_train,hue='PotentialFraud')
plt.xticks(size = 12) # size of x axis indicators(yes/no)
plt.yticks(size = 12) 
plt.title('Total_ip_op_amount_reimb vs IPAnnualReimbursementAmt', fontsize=20)
plt.xlabel('Total_ip_op_amount_reimb', size = 14)
plt.ylabel('IPAnnualReimbursementAmt', size = 14)
plt.show()

"""#### observation
1. showing linear behaviour

### Spliting data: train and validation data.
"""

# spliting data
y=final_data_train['PotentialFraud']
# # split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]]
X_train, X_cv, y_train, y_cv = train_test_split(final_data_train, y,stratify=y,test_size=0.33,random_state=42)
# print('Number of data points in train data:', X_train.shape)
# # print('Number of data points in test data:', X_test.shape[0])
# print('Number of data points in cross validation data:', X_cv.shape)





"""### Features Engineering on Splited data


#### 1.using Group by 

Our Main Aim is to find fraud by healthcare provider. So by grouping by provier we can get some useful infomation about a perticular provider.like its annual mean of reimbursement of a perticular provier it is more then the genuine provider then the provier must be suspected.

##### InscClaimAmtReimbursed
"""

#https://www.kaggle.com/rahuly93/medicare-provider-fraud-detection
#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html


# # we are getting mean of provider on x_train and X_test seprately to avoid data leakage proplem

mean_df=X_train[["InscClaimAmtReimbursed",'Provider']].groupby('Provider') # grouping the InsclaimAmtReimbursed on Provider
mean=mean_df.aggregate(np.mean) # getting mean of each group
provider_id=mean_df.groups # getting group names
g=list(provider_id.keys())
# adding mean of that group with the provider in new column 
from tqdm import tqdm   
for i,j in tqdm(zip(g,mean['InscClaimAmtReimbursed'])):
    X_train.loc[X_train['Provider'] == i, 'Mean_InscClaimAmtReimbursed'] = j

mean_df=X_cv[["InscClaimAmtReimbursed",'Provider']].groupby('Provider')
mean=mean_df.aggregate(np.mean)
provider_id=mean_df.groups
g=list(provider_id.keys())
from tqdm import tqdm   
for i,j in tqdm(zip(g,mean['InscClaimAmtReimbursed'])):
    X_cv.loc[X_cv['Provider'] == i, 'Mean_InscClaimAmtReimbursed'] = j

"""###### IPAnnualReimbursementAmt"""

mean_df=X_train[["IPAnnualReimbursementAmt",'Provider']].groupby('Provider')
mean=mean_df.aggregate(np.mean)
provider_id=mean_df.groups
g=list(provider_id.keys())

from tqdm import tqdm   
for i,j in tqdm(zip(g,mean['IPAnnualReimbursementAmt'])):
    X_train.loc[X_train['Provider'] == i, 'Mean_IPAnnualReimbursementAmt'] = j

mean_df=X_cv[["IPAnnualReimbursementAmt",'Provider']].groupby('Provider')
mean=mean_df.aggregate(np.mean)
provider_id=mean_df.groups
g=list(provider_id.keys())
from tqdm import tqdm   
for i,j in tqdm(zip(g,mean['IPAnnualReimbursementAmt'])):
    X_cv.loc[X_cv['Provider'] == i, 'Mean_IPAnnualReimbursementAmt'] = j

"""###### OPAnnualReimbursementAmt"""

mean_df=X_train[["OPAnnualReimbursementAmt",'Provider']].groupby('Provider')
mean=mean_df.aggregate(np.mean)
provider_id=mean_df.groups
g=list(provider_id.keys())

from tqdm import tqdm   
for i,j in tqdm(zip(g,mean['OPAnnualReimbursementAmt'])):
    X_train.loc[X_train['Provider'] == i, 'Mean_OPAnnualReimbursementAmt'] = j

mean_df=X_cv[["OPAnnualReimbursementAmt",'Provider']].groupby('Provider')
mean=mean_df.aggregate(np.mean)
provider_id=mean_df.groups
g=list(provider_id.keys())
from tqdm import tqdm   
for i,j in tqdm(zip(g,mean['OPAnnualReimbursementAmt'])):
    X_cv.loc[X_cv['Provider'] == i, 'Mean_OPAnnualReimbursementAmt'] = j

"""#### 2.Count featues

##### Getting Count of diffrent physician Attended a beneficiary
"""

# train_data
df_physician=X_train[['AttendingPhysician','OperatingPhysician','OtherPhysician']] # creating a new dataframe
c = np.where(df_physician==0,0,1) # replacing the physican code with 1
sum_total_featues=np.sum(c,axis=1) # adding all columns.

X_train['Total_physican_attended']=sum_total_featues # stroing in new column

# test
df_physician=X_cv[['AttendingPhysician','OperatingPhysician','OtherPhysician']]
c = np.where(df_physician==0,0,1)
sum_total_featues=np.sum(c,axis=1)

X_cv['Total_physican_attended']=sum_total_featues

"""##### #Getting Count of diffrent ClmDiagnosisCode"""

#train
li=[] # ceateing list of all ClmDiagonosisCode
for i in range(1,11):
    li.append("ClmDiagnosisCode_"+str(i))    
df_=X_train[li] # storing in a diffrent dataframe
c = np.where(df_==0,0,1) # Changing the code with 1 and 0.
sum_total_featues=np.sum(c,axis=1) # summing all 1's column vise.
X_train['Total_ClmDiagnosisCode']=sum_total_featues # storing in diffrent column

# test
li=[]
for i in range(1,11):
    li.append("ClmDiagnosisCode_"+str(i))    
df_=X_cv[li]             # storing in a diffrent dataframe
c = np.where(df_==0,0,1)  # Changing the code with 1 and 0.
sum_total_featues=np.sum(c,axis=1) 
X_cv['Total_ClmDiagnosisCode']=sum_total_featues

"""###### Getting Count of diffrent ClmProcedureCode"""

#train
li=[]  # ceateing list of all ClmProcedureCode
for i in range(1,7):
    li.append("ClmProcedureCode_"+str(i))    
df_=X_train[li]    # storing in a diffrent dataframe
c = np.where(df_==0,0,1)   # Changing the code with 1 and 0.
sum_total_featues=np.sum(c,axis=1)    # summing all 1's column vise
X_train['Total_ClmProcedureCode']=sum_total_featues

#test
li=[]
for i in range(1,7):
    li.append("ClmProcedureCode_"+str(i))    
df_=X_cv[li]
c = np.where(df_==0,0,1)
sum_total_featues=np.sum(c,axis=1)
X_cv['Total_ClmProcedureCode']=sum_total_featues

"""### 3.Categorical Featues

As our main motive to find fraud healthcare provider. Therefor we are getting top categories which belongs to the fraud provider and doing one hot encoding for this we are using only Train data to avoid data leakage problem.
"""

# # creating dataframe which only contains fraud data.
fraud_data=X_train[X_train['PotentialFraud']==1] # from trian data only.

"""##### ClmAdmitDiagnosisCode
###### Top 20 categories belong to fraud providers
"""

claim_adimt_code_train=fraud_data['ClmAdmitDiagnosisCode'].value_counts()[:21] # only top 20 code exculding 0/nan values
# changing codes in trian data if they claim_admit code present in the column or not if not replace it by none
top_20_claim_admit_code=claim_adimt_code_train.keys()
claim_code_allowed_train=list(top_20_claim_admit_code)
print(claim_code_allowed_train)

if 0 in claim_code_allowed_train:  # Removing zero form the list. 
    claim_code_allowed_train.remove(0)
for i in claim_code_allowed_train:
#     print(i)
# adding new featues with that category to the  train data.
# replacing that code with 1 and other with 0. 
    X_train['ClmAdmitDiagnosisCode_'+i]=np.where(X_train["ClmAdmitDiagnosisCode"].str.contains(i), 1, 0)

claim_adimt_code_train=fraud_data['ClmAdmitDiagnosisCode'].value_counts()[:21] # only top 20 code exculding 0/nan values
# changing codes in trian data if they claim_admit code present in the column or not if not replace it by none
top_20_claim_admit_code=claim_adimt_code_train.keys()
claim_code_allowed_train=list(top_20_claim_admit_code)
print(claim_code_allowed_train)

if 0 in claim_code_allowed_train:
    claim_code_allowed_train.remove(0)
for i in claim_code_allowed_train:
#     print(i)
    X_cv['ClmAdmitDiagnosisCode_'+i]=np.where(X_cv["ClmAdmitDiagnosisCode"].str.contains(i), 1, 0)

"""###### ClmDiagnosisCode
###### Top 10 categories belong to fraud providers
"""

for i in range(1,11):
    code=fraud_data['ClmDiagnosisCode_'+str(i)].value_counts()[:10]  # craetind list of code for each ClmDiagnosisCode column
    code=code.keys()
    code=list(code)
    if 0 in code: # removing code zero. if occure
        code.remove(0)
    for k in code:

        X_train['ClmDiagnosisCode_'+k]=np.where(X_train["ClmDiagnosisCode_"+str(i)].str.contains(k), 1, 0) # replacing the code with 1. and other values with zero.

for i in range(1,11):
    code=fraud_data['ClmDiagnosisCode_'+str(i)].value_counts()[:10]
    code=code.keys()
    code=list(code)
    if 0 in code:
        code.remove(0)
    for k in code:

        X_cv['ClmDiagnosisCode_'+k]=np.where(X_cv["ClmDiagnosisCode_"+str(i)].str.contains(k), 1, 0)
        
print(X_train.shape,X_cv.shape)

"""##### AttendingPhysician
###### Top 10 categories belong to fraud providers
"""

claim_adimt_code_train=fraud_data['AttendingPhysician'].value_counts()[:10] # only top 20 code exculding 0/nan values
# changing codes in trian data if they claim_admit code present in the column or not if not replace it by none
top_20_claim_admit_code=claim_adimt_code_train.keys()
claim_code_allowed_train=list(top_20_claim_admit_code)
print(claim_code_allowed_train)
if 0 in claim_code_allowed_train:
    
    claim_code_allowed_train.remove(0)
for i in claim_code_allowed_train:
#     print(i)
# replacing the code with 1. and other values with zero.
    X_train['AttendingPhysician_'+i]=np.where(X_train["AttendingPhysician"].str.contains(i), 1, 0)

claim_adimt_code_train=fraud_data['AttendingPhysician'].value_counts()[:10] # only top 20 code exculding 0/nan values
# changing codes in trian data if they claim_admit code present in the column or not if not replace it by none
top_20_claim_admit_code=claim_adimt_code_train.keys()
claim_code_allowed_train=list(top_20_claim_admit_code)
print(claim_code_allowed_train)
if 0 in claim_code_allowed_train:
    
    claim_code_allowed_train.remove(0)
for i in claim_code_allowed_train:
#     print(i)
    X_cv['AttendingPhysician_'+i]=np.where(X_cv["AttendingPhysician"].str.contains(i), 1, 0)
print(X_train.shape,X_cv.shape)

"""##### OperatingPhysician
###### Top 10 categories belong to fraud providers
"""

claim_adimt_code_train=fraud_data['OperatingPhysician'].value_counts()[:10] # only top 20 code exculding 0/nan values
# changing codes in trian data if they claim_admit code present in the column or not if not replace it by none
top_20_claim_admit_code=claim_adimt_code_train.keys()
claim_code_allowed_train=list(top_20_claim_admit_code)
print(claim_code_allowed_train)
if 0 in claim_code_allowed_train:
    claim_code_allowed_train.remove(0)
for i in claim_code_allowed_train:
    print(i)
    X_train['OperatingPhysician_'+i]=np.where(X_train["OperatingPhysician"].str.contains(i), 1, 0)


print(X_train.shape,X_cv.shape)



claim_adimt_code_train=fraud_data['OperatingPhysician'].value_counts()[:10] # only top 20 code exculding 0/nan values
# changing codes in trian data if they claim_admit code present in the column or not if not replace it by none
top_20_claim_admit_code=claim_adimt_code_train.keys()
claim_code_allowed_train=list(top_20_claim_admit_code)
print(claim_code_allowed_train)
if 0 in claim_code_allowed_train:
    claim_code_allowed_train.remove(0)
for i in claim_code_allowed_train:
    print(i)
    X_cv['OperatingPhysician_'+i]=np.where(X_cv["OperatingPhysician"].str.contains(i), 1, 0)


print(X_train.shape,X_cv.shape)

"""##### OtherPhysician
###### Top 10 categories belong to fraud providers
"""

claim_adimt_code_train=fraud_data['OtherPhysician'].value_counts()[:10] # only top 20 code exculding 0/nan values
# changing codes in trian data if they claim_admit code present in the column or not if not replace it by none
top_20_claim_admit_code=claim_adimt_code_train.keys()
claim_code_allowed_train=list(top_20_claim_admit_code)
print(claim_code_allowed_train)
if 0 in claim_code_allowed_train:
    claim_code_allowed_train.remove(0)
for i in claim_code_allowed_train:
    print(i)
    X_train['OtherPhysician_'+i]=np.where(X_train["OtherPhysician"].str.contains(i), 1, 0)

claim_adimt_code_train=fraud_data['OtherPhysician'].value_counts()[:10] # only top 20 code exculding 0/nan values
# changing codes in trian data if they claim_admit code present in the column or not if not replace it by none
top_20_claim_admit_code=claim_adimt_code_train.keys()
claim_code_allowed_train=list(top_20_claim_admit_code)
print(claim_code_allowed_train)
if 0 in claim_code_allowed_train:
    claim_code_allowed_train.remove(0)
for i in claim_code_allowed_train:
    print(i)
    X_cv['OtherPhysician_'+i]=np.where(X_cv["OtherPhysician"].str.contains(i), 1, 0)
    
print(X_train.shape,X_cv.shape)

"""#### 4.Diffrence
Calculating the diffrence betweet healthcare fraud provder maximun amount to the amount of beneficiary.
"""

# fraud_data=X_train[X_train['PotentialFraud']==1]
print('Maximum IPAnnualreimbursementAmt:-',fraud_data['IPAnnualReimbursementAmt'].max())
print( 'Maximum OPAnnualReimbursementAmt:-',fraud_data['OPAnnualReimbursementAmt'].max())
print('Maximum InscClaimAmtReimbursed:-',fraud_data['InscClaimAmtReimbursed'].max())

#Diff_max_IPAnnualReimbursementAm
X_train['Diff_max_IPAnnualReimbursementAmt']=fraud_data['IPAnnualReimbursementAmt'].max()-X_train['IPAnnualReimbursementAmt']
# OPAnnualReimbursementAmt
X_train['Diff_max_OPAnnualReimbursementAmt']=fraud_data['OPAnnualReimbursementAmt'].max()-X_train['OPAnnualReimbursementAmt']
#InscClaimAmtReimbursed
X_train['Diff_max_InscClaimAmtReimbursed']=fraud_data['InscClaimAmtReimbursed'].max()-X_train['InscClaimAmtReimbursed']

print(X_train.shape,X_cv.shape)

# #deviation
# fraud_data=X_cv[X_cv['PotentialFraud']==1]
print('Maximum IPAnnualreimbursementAmt:-',fraud_data['IPAnnualReimbursementAmt'].max())
print( 'Maximum OPAnnualReimbursementAmt:-',fraud_data['OPAnnualReimbursementAmt'].max())

#IPAnnualReimbursementAmt
X_cv['Diff_max_IPAnnualReimbursementAmt']=fraud_data['IPAnnualReimbursementAmt'].max()-X_cv['IPAnnualReimbursementAmt']
# OpannualReimbursemetamt
X_cv['Diff_max_OPAnnualReimbursementAmt']=fraud_data['OPAnnualReimbursementAmt'].max()-X_cv['OPAnnualReimbursementAmt']
#InscClaimAmtReimbursed
X_cv['Diff_max_InscClaimAmtReimbursed']=fraud_data['InscClaimAmtReimbursed'].max()-X_cv['InscClaimAmtReimbursed']

print(X_train.shape,X_cv.shape)

"""### Handling Categorical data"""

# for country one hot encoding.
contry_code=X_train['County'].unique()
# contry_code=map(str,contry_code)
for i in contry_code:
    X_train['County_'+str(i)]=np.where(X_train["County"]==i, 1, 0)
# test data

for i in contry_code:
    
    X_cv['County_'+str(i)]=np.where(X_cv["County"]==i, 1, 0)

# Convert type of Gender and Race to categorical
X_train.Gender=X_train.Gender.astype('category')
X_cv.Gender=X_cv.Gender.astype('category')

X_train.Race=X_train.Race.astype('category')
X_cv.Race=X_cv.Race.astype('category')

X_train.State=X_train.State.astype('category')
# X_train.County=X_train.County.astype('category')

X_cv.State=X_cv.State.astype('category')
# X_cv.County=X_cv.County.astype('category')

X_train.NoOfMonths_PartACov=X_train.NoOfMonths_PartACov.astype('category')
X_train.NoOfMonths_PartBCov=X_train.NoOfMonths_PartBCov.astype('category')

X_cv.NoOfMonths_PartACov=X_cv.NoOfMonths_PartACov.astype('category')
X_cv.NoOfMonths_PartBCov=X_cv.NoOfMonths_PartBCov.astype('category')
# Do one hot encoding for gender and Race
X_train=pd.get_dummies(X_train,columns=['Gender','Race','State','NoOfMonths_PartBCov','NoOfMonths_PartACov'])

X_cv=pd.get_dummies(X_cv,columns=['Gender','Race','State','NoOfMonths_PartBCov','NoOfMonths_PartACov'])

print(X_train.shape,X_cv.shape)

"""### Droping  features
droping features like DOB,DOD etc. 
"""

remove_col=['AttendingPhysician','OperatingPhysician','OtherPhysician','ClmAdmitDiagnosisCode','DiagnosisGroupCode',
           'Provider','BeneID', 'ClaimID', 'ClaimStartDt','ClaimEndDt',"DOD",'DOB','AdmissionDt','County',
           'DischargeDt','Provider','County']
for i in range(1,11):
    remove_col.append('ClmDiagnosisCode_'+str(i))
for i in range(1,7):
    remove_col.append('ClmProcedureCode_'+str(i))
    
    
print(remove_col)

X_train=X_train.drop(columns=remove_col,axis=1)
X_cv=X_cv.drop(columns=remove_col,axis=1) 
print(X_train.shape,X_cv.shape)

"""### Creating diffrent datasets
1. Final dataset With outliers 
2. Final dataset Wihtout outliers
3. Final dateset Without outliers and colliner Featues
4. only grouped featues.

#### 1.with outliers --orignal data
As for predicting  fraud healthcare provider Outliers may play important role. so we are creating a dataset withoutlier
"""

print(X_train.shape,X_cv.shape)

"""#### 2.Final dateset Without outliers and colliner Featues
By the EDA we got some collinear and outliers now we are removing them.
1. Admitted days are collinear with deductible amount paid
2. InscClaimAmountReimbursed with Amount get.
3. Total_in_out_anual_reimbrused with IPanualAmtReimbrused.
4. IPAnnualDeductibleAmt is very collinear  with IPAnnualReimbursementAmt with 0.97

removing one of the Colliner features.
"""

# removing values greater then 99.8 percentile
list_1=['IPAnnualDeductibleAmt','IPAnnualDeductibleAmt','OPAnnualDeductibleAmt','OPAnnualReimbursementAmt',
 'InscClaimAmtReimbursed','Total_ip_op_amount_reimb','total_ip_op_amount_deduct']
for i in list_1:
    X_train_w=X_train[X_train[i]<np.percentile(X_train[i],99.8)]
    X_cv_w=X_cv[X_cv[i]<np.percentile(X_cv[i],99.8)]

# # remvoing collinear featues

col=['Admitted_days','Amount_get','IPAnnualReimbursementAmt']
X_train_w=X_train_w.drop(columns=col,axis=1)
X_cv_w=X_cv_w.drop(columns=col,axis=1)

y_train_w=X_train_w['PotentialFraud']
y_cv_w=X_cv_w['PotentialFraud']
X_train=X_train.drop(columns=['PotentialFraud'],axis=1)
X_cv=X_cv.drop(columns=['PotentialFraud'],axis=1)


# saving in csv_files
X_train_w=X_train_w.drop(columns=['PotentialFraud'],axis=1)
X_cv_w=X_cv_w.drop(columns=['PotentialFraud'],axis=1) 

X_train_w.to_csv('X_train_w.csv')
X_cv_w.to_csv('X_cv_w.csv')
y_train_w.to_csv('y_train_w.csv')
y_cv_w.to_csv('y_cv_w.csv')
print(X_train_w.shape,X_cv_w.shape)


X_train.to_csv('X_train.csv')
y_train.to_csv('y_train.csv')
X_cv.to_csv('X_cv.csv')
y_cv.to_csv('y_cv.csv')











